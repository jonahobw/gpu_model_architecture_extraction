"""Performs data cleaning and preprocessing on aggregated profile data in CSV format.

This module provides functionality for data engineering tasks on GPU profiling data, including:
- Filling missing values
- Detecting complete, partial, or unique features by model
- Filtering data by system metrics, GPU activities, or API calls
- Splitting data into training and test sets
- Adding indicator columns for missing values

The module operates on aggregated CSV files generated by format_profiles.py.

Dependencies:
    - pandas: For data manipulation
    - numpy: For numerical operations
    - config: For configuration constants
    - format_profiles: For reading CSV data

Example Usage:
    ```python
    from data_engineering import shared_data, all_data
    
    # Get complete features for machine learning
    df = shared_data("profiles_folder", system_data_only=True)
    
    # Get all features with indicator columns
    df = all_data("profiles_folder", indicators_only=True)
    ```
"""
import json
from pathlib import Path
from typing import Dict, List, Tuple, Union, Optional

import numpy as np
import pandas as pd

import config
from format_profiles import read_csv


def subsample(df: pd.DataFrame, num: int, col: str = "model") -> pd.DataFrame:
    """Return a dataframe with the first n rows from each value in specified column.
    
    Args:
        df: Input DataFrame
        num: Number of rows to sample per group
        col: Column to group by (default: "model")
        
    Returns:
        DataFrame with n rows per group from specified column
    """
    return df.groupby(col).head(num).reset_index(drop=True).copy(deep=True)


def removeColumnsFromOther(keep_cols: pd.DataFrame, remove_df: pd.DataFrame) -> pd.DataFrame:
    """Remove columns from remove_df that are not present in keep_cols.
    
    Args:
        keep_cols: DataFrame containing columns to keep
        remove_df: DataFrame to remove columns from
        
    Returns:
        DataFrame with only columns present in keep_cols
    """
    to_remove = [x for x in remove_df.columns if x not in keep_cols.columns]
    return remove_cols(remove_df, to_remove)


def softmax(x: np.ndarray) -> np.ndarray:
    """Compute softmax values for each set of scores in x.
    
    Args:
        x: Input array of scores
        
    Returns:
        Array of softmax probabilities
    """
    return np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum()


def remove_cols(
    df: pd.DataFrame, 
    substrs: Union[str, List[str]], 
    endswith: bool = False, 
    verbose: bool = False
) -> pd.DataFrame:
    """Remove columns containing (or ending with) specified substrings.
    
    Args:
        df: Input DataFrame
        substrs: String or list of strings to match in column names
        endswith: If True, match end of column names instead of anywhere
        verbose: If True, print removed column names
        
    Returns:
        DataFrame with matching columns removed
    """
    if not isinstance(substrs, list):
        substrs = [substrs]

    def remove_col(col_name: str) -> bool:
        for substr in substrs:
            if not endswith and substr in col_name:
                return True
            if endswith and col_name.endswith(substr):
                return True
        return False

    remove_columns = [col_name for col_name in df.columns if remove_col(col_name)]

    if verbose:
        print("\nRemoving columns:")
        for i in remove_columns:
            print(i)

    return df.drop(remove_columns, axis=1)


def filter_cols(
    df: pd.DataFrame, 
    substrs: List[str], 
    keep: Optional[List[str]] = None, 
    verbose: bool = False
) -> pd.DataFrame:
    """Keep only columns containing specified substrings.
    
    Args:
        df: Input DataFrame
        substrs: List of strings to match in column names
        keep: List of column names to always keep (default: ["model", "model_family", "file"])
        verbose: If True, print removed column names
        
    Returns:
        DataFrame with only matching columns
    """
    if keep is None:
        keep = ["model", "model_family", "file"]

    def keep_col(col_name: str) -> bool:
        for substr in substrs:
            if substr in col_name:
                return True
        for x in keep:
            if col_name == x:
                return True
        return False

    remove_columns = [col_name for col_name in df.columns if not keep_col(col_name)]
    if verbose:
        print("\nRemoving columns:")
        for i in remove_columns:
            print(i)

    return df.drop(remove_columns, axis=1)


def get_csv(
    aggregated_csv_folder: str,
    remove_nans: bool = True,
    gpu_activities_only: bool = False,
    api_calls_only: bool = False
) -> pd.DataFrame:
    """Read and preprocess aggregated CSV data.
    
    Args:
        aggregated_csv_folder: Path to folder containing aggregated.csv
        remove_nans: If True, remove columns containing NaN values
        gpu_activities_only: If True, keep only GPU activity columns
        api_calls_only: If True, keep only API call columns
        
    Returns:
        Processed DataFrame
    """
    df = read_csv(
        aggregated_csv_folder,
        gpu_activities_only=gpu_activities_only,
        api_calls_only=api_calls_only,
    )
    if remove_nans:
        df = remove_cols(df, "nan", verbose=True)
    return df


def missing_data(
    aggregated_csv_folder: str,
    gpu_activities_only: bool = False,
    api_calls_only: bool = False
) -> Dict[str, pd.Series]:
    """Get missing data statistics by model.
    
    Args:
        aggregated_csv_folder: Path to folder containing aggregated.csv
        gpu_activities_only: If True, analyze only GPU activity columns
        api_calls_only: If True, analyze only API call columns
        
    Returns:
        Dictionary mapping models to Series of missing data percentages by column in the aggregated csv.
    """
    df = get_csv(
        aggregated_csv_folder,
        gpu_activities_only=gpu_activities_only,
        api_calls_only=api_calls_only,
    )
    model_nans = {}

    for model in df["model"].unique():
        model_df = df.loc[df["model"] == model]
        n_model_profiles = len(model_df.index)
        model_nans[model] = model_df.isna().sum() / n_model_profiles

    # model_nans is now a dict of {model_type: Pandas series of {column_name: % of that column that is empty}}
    return model_nans


def mutually_exclusive_data(
    aggregated_csv_folder: str,
    gpu_activities_only: bool = False,
    api_calls_only: bool = False
) -> Dict[str, Union[Dict[str, List[str]], List[str]]]:
    """Analyze feature completeness and exclusivity across models.
    
    Args:
        aggregated_csv_folder: Path to folder containing aggregated.csv
        gpu_activities_only: If True, analyze only GPU activity columns
        api_calls_only: If True, analyze only API call columns
        
    Returns:
        Dictionary containing:
        - mutually_exclusive_attributes: the attributes which are exclusive to each model.  Only include an attribute
          if each model profile includes this attribute and none of the others include it.
        - partial_attributes: attributes which models have for some profiles but not others.
        - no_data_attributes: list of attributes for which there is no data from any model.
        - complete_attributes: A list of attributes which each model has completely.
        - partially_exclusive_attributes: A mapping from attributes to models for attributes which some but
          not all models have.
    """
    model_nans = missing_data(
        aggregated_csv_folder,
        gpu_activities_only=gpu_activities_only,
        api_calls_only=api_calls_only,
    )
    num_models = len(list(model_nans.keys()))

    mutually_exclusive = {model: [] for model in model_nans}
    partial_attribute = {model: [] for model in model_nans}
    attributes_with_no_data = []
    complete_attributes = []
    partially_exclusive = {}

    for attribute in model_nans[next(iter(model_nans))].axes[0]:
        models_with_complete_attribute = []
        models_with_partial_attribute = []
        models_without_attribute = []
        for model in model_nans:
            percent_empty = model_nans[model][attribute]
            if percent_empty == 0.0:
                models_with_complete_attribute.append(model)
            if 0.0 < percent_empty < 1.0:
                models_with_partial_attribute.append(model)
                partial_attribute[model].append(attribute)
            if percent_empty == 1.0:
                models_without_attribute.append(model)
        if (
            len(models_with_complete_attribute) == 1
            and len(models_with_partial_attribute) == 0
        ):
            mutually_exclusive[models_with_complete_attribute[0]].append(attribute)
        if len(models_without_attribute) == num_models:
            attributes_with_no_data.append(attribute)
        if len(models_with_complete_attribute) == num_models:
            complete_attributes.append(attribute)
        if (
            0 < len(models_with_complete_attribute) < num_models
            and len(models_without_attribute) > 0
        ):
            partially_exclusive[attribute] = models_with_complete_attribute

    return {
        "mutually_exclusive_attributes": mutually_exclusive,
        "partial_attributes": partial_attribute,
        "no_data_attributes": attributes_with_no_data,
        "complete_attributes": complete_attributes,
        "partially_exclusive_attributes": partially_exclusive,
    }


def shared_data(
    agg_csv_folder: str,
    system_data_only: bool = False,
    no_system_data: bool = False,
    gpu_activities_only: bool = False,
    api_calls_only: bool = False
) -> pd.DataFrame:
    """Get complete features suitable for machine learning.
    
    Args:
        agg_csv_folder: Path to folder containing aggregated.csv
        system_data_only: If True, keep only system metrics
        no_system_data: If True, exclude system metrics
        gpu_activities_only: If True, keep only GPU activity columns
        api_calls_only: If True, keep only API call columns
        
    Returns:
        DataFrame with complete features
        
    Raises:
        ValueError: If both system_data_only and no_system_data are True
    """
    if system_data_only and no_system_data:
        raise ValueError("system_data_only and no_system_data cannot both be true.")

    df = get_csv(agg_csv_folder)
    complete_attributes = mutually_exclusive_data(
        agg_csv_folder,
        gpu_activities_only=gpu_activities_only,
        api_calls_only=api_calls_only,
    )["complete_attributes"]
    df = df[complete_attributes]

    if not system_data_only and not no_system_data:
        return df

    def system_column(col: str) -> bool:
        for sys_signal in config.SYSTEM_SIGNALS:
            if col.endswith(sys_signal):
                return True
        return False

    system_cols = [
        col_name for col_name in complete_attributes if system_column(col_name)
    ]

    if system_data_only:
        system_cols.append("model")
        system_cols.append("model_family")
        system_cols.append("file")
        return df[system_cols]

    return df.drop(system_cols, axis=1)


def train_test_split(df: pd.DataFrame, ratio: float = 0.8) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Split data into train and test sets with balanced class representation.
    
    Args:
        df: Input DataFrame
        ratio: Train/test split ratio (default: 0.8)
        
    Returns:
        Tuple of (train_df, test_df)
    """
    test_df = pd.DataFrame()
    train_df = pd.DataFrame()
    for model in config.MODELS:
        model_rows = df[df["model"] == model]
        num_rows = len(model_rows.index)

        train_rows = int(num_rows * ratio)
        train_df = pd.concat([train_df, model_rows.head(train_rows)], ignore_index=True)

        test_rows = num_rows - train_rows
        test_df = pd.concat([test_df, model_rows.tail(test_rows)], ignore_index=True)

    return train_df, test_df


def get_data_and_labels(
    df: pd.DataFrame,
    shuffle: bool = True,
    label: Optional[str] = None
) -> Tuple[pd.DataFrame, pd.Series]:
    """Split DataFrame into features and labels.
    
    Args:
        df: Input DataFrame
        shuffle: If True, shuffle the data
        label: Column to use as label (default: "model")
        
    Returns:
        Tuple of (features, labels)
    """
    if shuffle:
        df = df.sample(frac=1)

    if not label:
        y = df["model"]
    else:
        y = df[label]

    x = df.drop(columns=["file", "model_family", "model"], axis=1)

    return x, y


def add_indicator_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Add binary indicator columns for missing values.
    
    For each column with NaN values, adds a binary indicator column and fills
    missing values with the mean. Since GPU data is split into 6 features (min, max,
    avg, num_calls, time_ms, time_percent), only 1 indicator column is added instead of 6.
    
    Args:
        df: Input DataFrame
        
    Returns:
        DataFrame with indicator columns and filled missing values
        
    Raises:
        RuntimeError: If NaN values remain after processing
    """
    prefixes = [
        "min_us_",
        "max_ms_",
        "avg_us_",
        "num_calls_",
        "time_ms_",
        "time_percent_",
    ]

    def stripPrefix(col_name: str) -> str:
        for prefix in prefixes:
            if col_name.startswith(prefix):
                return col_name[len(prefix):]
        raise ValueError

    for col in df.columns:
        column = df[col]
        if column.isna().sum() > 0:
            indicator_col = column.notna().astype(int)
            indicator_col.name = f"indicator_{stripPrefix(col)}"
            mean = column.mean()
            df[col] = df[col].fillna(mean)
            if indicator_col.name in df.columns:
                continue
            df = pd.concat([df, indicator_col], axis=1)
        if df[col].isna().sum() > 0:
            raise RuntimeError(f"Still NaNs in column {col}")
    return df


def add_indicator_cols_to_input(
    df: pd.DataFrame,
    x: pd.Series,
    exclude: List[str] = []
) -> pd.Series:
    """Modify input data columns to match reference DataFrame.

    - Adds indicator columns to the input data to match the reference DataFrame.
    - Fills missing values in the input data with the mean of the column.
    - Drops columns from the input data that are not in the reference DataFrame.
    - Excludes columns from the input data that are in the exclude list.
    
    Args:
        df: Reference DataFrame
        x: Input Series to process
        exclude: List of columns to exclude
        
    Returns:
        Processed Series with matching columns and indicators
    """
    for col in df.columns:
        # only look at indicator columns in the reference df
        if col in exclude or not col.startswith("indicator_"):
            continue
        meta_feature = col.split("indicator_")[1]
        found = False
        for col_name in x.keys():
            if col_name.find(meta_feature) >= 0:
                x[col] = 1
                found = True
                break
        if not found:
            x[col] = 0

    # now all indicator columns are added, fill in missing
    # values with the mean
    for col in df.columns:
        if col in exclude or col.startswith("indicator_"):
            continue
        if col not in x.keys():
            x[col] = df[col].mean()

    for i in x.keys():
        if i not in df.columns:
            x = x.drop(i)

    # sort x in order of the keys of the df.
    x = pd.DataFrame([x])
    new_df = pd.concat((df, x), ignore_index=True)

    result = new_df.iloc[-1]
    result = result.drop(exclude)
    return result


def all_data(
    agg_csv_folder: str,
    system_data_only: bool = False,
    no_system_data: bool = False,
    indicators_only: bool = False,
    gpu_activities_only: bool = False,
    api_calls_only: bool = False
) -> pd.DataFrame:
    """Get all features with indicator columns for missing values.
    
    Args:
        agg_csv_folder: Path to folder containing aggregated.csv
        system_data_only: If True, keep only system metrics
        no_system_data: If True, exclude system metrics
        indicators_only: If True, keep only indicator columns
        gpu_activities_only: If True, keep only GPU activity columns
        api_calls_only: If True, keep only API call columns
        
    Returns:
        Processed DataFrame
        
    Raises:
        ValueError: If both system_data_only and no_system_data are True
        ValueError: If both indicators_only and system_data_only are True
    """
    if system_data_only and no_system_data:
        raise ValueError("system_data_only and no_system_data cannot both be true.")

    df = get_csv(
        agg_csv_folder,
        gpu_activities_only=gpu_activities_only,
        api_calls_only=api_calls_only,
    )

    df = add_indicator_columns(df)

    if indicators_only and system_data_only:
        raise ValueError

    if indicators_only:
        indicator_cols = [col for col in df.columns if col.startswith("indicator")]
        indicator_cols.append("model")
        indicator_cols.append("model_family")
        indicator_cols.append("file")
        return df[indicator_cols]

    if not system_data_only and not no_system_data:
        return df

    def system_column(col: str) -> bool:
        for sys_signal in config.SYSTEM_SIGNALS:
            if col.endswith(sys_signal):
                return True
        return False

    system_cols = [col_name for col_name in df.columns if system_column(col_name)]

    if system_data_only:
        system_cols.append("model")
        system_cols.append("model_family")
        system_cols.append("file")
        return df[system_cols]

    return df.drop(system_cols, axis=1)
